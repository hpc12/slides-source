-------------------------------------------------------------------------------
valgrind demo
-------------------------------------------------------------------------------
valgrind
  Finding crash bugs before they cause crashes?
  memcheck
  -----------------------------------------------------------------------------
  FIRST TYPE OF BUG: Uninitialized values
    valgrind matmul (NOTHING)

  Add "-g"
  Re-point out antagonism of -O and -g

  DEBUG_FLAGS=-g make

  Check unintialized line
  valgrind --track-origins=yes matmul 5

  -----------------------------------------------------------------------------
  SECOND TYPE OF BUG: Out-of
    valgrind matmul 5

  but:
    doesn't show for what indices!
    valgrind --db-attach=yes
    (depends on -g)
  -----------------------------------------------------------------------------
  THIRD TYPE OF BUG: Leaks
    valgrind matmul 5
    valgrind --leak-check=full matmul 5
  -----------------------------------------------------------------------------
  TODO: Usage with crappy/highly optimized code (CL implementations, Python)

  valgrind --suppressions=opencl.supp ./cl-demo 1000 10
  Add strcmp suppression
  too broad
  go up in stack with C++ function name
  doesn't work
  --demangle
  add a C++ suppression
  -----------------------------------------------------------------------------
  Mention MPI
  -----------------------------------------------------------------------------
  TOOL: helgrind
  - Need to recompile gcc with special flag
  - Next week's "tool of the week": Compiling your own GNU-ish software
  -----------------------------------------------------------------------------
  But wait, there's more:
  valgrind --tool=callgrind ./cl-demo 1000 10

  Forward pointer: Once we know what caches are, you can also gather simulated
    cache behavior

  valgrind --tool=cachegrind ./cl-demo 1000 10
-------------------------------------------------------------------------------
mpi demo cont'd
-------------------------------------------------------------------------------
mpi-periodic-send2:

- evolve mpi-nonblock
- Now what if everybody needs to send two bits of data?
- run on 10 nodes, pipe to file
- for i in $(seq 0 9); do echo $i; grep "dest-rank $i" a; done

Potential crash/deadlock/confusion:
- If msg1 and msg2 are of very different nature
  (say, in different subroutines)
  Possible fixes? (Barrier, targeted sends and receives--but can you be sure?)

< Non-overtaking

- Try barriers as a fix.
- Try targeted sends and receives.
-------------------------------------------------------------------------------
a glimpse at performance:

- two key figures? latency/bw

BANDWIDTH:
- bandwidth? walk through benchmark, noting benchmarking tricks:
  "loop": run how often
  "skip": skip the warm-up
  "window-size": keep how many non-blocking requests in flight

- run just on box
  as good as 'memcpy'
  very near memory bandwidth! How many extra buffer copies can there be?
  -> point of all this buffer nonsense

  Implementation matters:
  Previous example uses shared memory between processes
  mpiexec --mca btl self,tcp -n 2
  You see the extra buffer copies!

  Now -H box,slate
  -> Networks slow
  -> cardiac,bowery use faster interconnect, I'd encourage you to try this code
     there and compare with our results

BI-BANDWIDTH:
  quick walkthrough
  full-duplex or aggregate bandwidth?

LATENCY:
  quick walkthrough
  on-host
  on-host tcp
  off-host
-------------------------------------------------------------------------------
< spam (collectives)

- evolve mpi-hello
  MPI_Bcast(buffer, count, type, root, comm)

-------------------------------------------------------------------------------
MPE Demo:

- make mpe-periodic-send2-soln
- note how to compile
- run jumpshot on log file
- click through conversion
- point out legend window
-------------------------------------------------------------------------------
