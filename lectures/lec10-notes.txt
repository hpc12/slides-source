-------------------------------------------------------------------------------
False sharing demo
-------------------------------------------------------------------------------
ON SLATE:
- set-governor
- show code
- ./run-threads-vs-cache
- why?!
- fix it
- rerun
- "false sharing"
-------------------------------------------------------------------------------
NUMA Demo
-------------------------------------------------------------------------------
ON CRUNCHY3:
- show code
- have to pin thread -> core
  - OTHERWISE NOTHING MAKES ANY SENSE
- have to make sure memory is on determined node
  - alternative: "first touch"

- re
- ./make-on-crunchy
- show results
- discuss locality matrix
- single bw: NU for real
- contention
  - compare to zero contention case: that's like single-local
- all-contention
- two-contention
-------------------------------------------------------------------------------
Lock contention demo
-------------------------------------------------------------------------------
- set-governor
- Run with a single thread
- Put rate into perspective with sin() -- remove
- Run with 1,2,3,4,5,...,all threads
-------------------------------------------------------------------------------
GPU Branching Demo
-------------------------------------------------------------------------------
BOX
- show code
- run with 32
- run with 16 (just comment out cases)
- comment on how implemented
- run with 8
- ...
- run with 1
-------------------------------------------------------------------------------
GPU latency demo
-------------------------------------------------------------------------------
BOX:
- show code
- shrink work group size
- EXPLANATION: less latency hiding
- Use 2x ILP
  - halve size
  - halve chunksize
  - add extra fetch/store
- Use 4x ILP
-------------------------------------------------------------------------------
Arch understanding demo
-------------------------------------------------------------------------------
- What the hell is a warp?
- Why is the size of the register file given in kiB?

- mem bw
  bus bits * bus clock /1e3 / 8

-  flops per core, per clock
  fpus * 2

- flop rate [gflops]
  cores * core clock * fpus * 2 * 1e6 / 1e9

- how many scheduling slots per core?
  warp size * (# warps/core)

- how many scheduling slots total?
  * cores
  -> and that's just what the hardware does!

- how much register file per work item?
  -> "I'm going to make a mistake."
  reg file * 1024 / # fpus

  correct:
  reg file * 1024 / # work items

- smem bw / WHAT?
  -> FLOP!

  lmem bw / (#fpus * 2)

- gmem bw / flop?

  (gmem bw *1e9) / (#cores) / (core clock *1e6) / (#fpus * 2)
-------------------------------------------------------------------------------
Occupancy calculator demo
-------------------------------------------------------------------------------
Use gnumeric
- grow work group size up to 256
- observe: occupancy increases
- then increase lmem to 16384
- observe: occupancy drops

- Is occupancy absolutely necessary?
  (No. ILP can be just as good. But you pay with space in the reg.file.)
-------------------------------------------------------------------------------
Gmem access patterns demo
-------------------------------------------------------------------------------
- +i alignment on 590 -- 0 1 2 3 16
- *i strides on 295 -- 0 1 2 4 8 16 32
-------------------------------------------------------------------------------
Lmem access patterns demo
-------------------------------------------------------------------------------
FIXME!

- +i alignment on 590 -- 0 1 2 3 16
- *i strides on 295 -- 0 1 2 4 8 16 32
-------------------------------------------------------------------------------
a glimpse at MPI performance:
-------------------------------------------------------------------------------

- two key figures? latency/bw

BANDWIDTH:
- bandwidth? walk through benchmark, noting benchmarking tricks:
  "loop": run how often
  "skip": skip the warm-up
  "window-size": keep how many non-blocking requests in flight

- run just on box
  as good as 'memcpy'
  very near memory bandwidth! How many extra buffer copies can there be?
  -> point of all this buffer nonsense

  Implementation matters:
  Previous example uses shared memory between processes
  mpiexec --mca btl self,tcp -n 2
  You see the extra buffer copies!

  Now -H box,slate
  -> Networks slow
  -> cardiac,bowery use faster interconnect, I'd encourage you to try this code
     there and compare with our results

BI-BANDWIDTH:
  quick walkthrough
  full-duplex or aggregate bandwidth?

LATENCY:
  quick walkthrough
  on-host
  on-host tcp
  off-host

