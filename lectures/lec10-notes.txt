-------------------------------------------------------------------------------
sysprof demo
-------------------------------------------------------------------------------
- show UI
- just hit start / profile
- show sample counter
- no influence on rate
- clear, explain clear
- useful idea--will encounter another profiler like that

- set-governor
- cd fish
- mpiexec -n 4 ./fish <nfish> <vis_int> <nsteps>
- mpiexec -n 4 ./fish 1,000,000 0 30

- How does it know callers/callees?
- Switch to -g
- Explain conflict

- Re-mpiexec -n 4 ./fish 1,000,000 0 30
- Not a big difference in run time. Why? (mem bound)
- Self, cumulative times

- Trustworthiness of low-sample-count data

- Now know *where* time is spent. Don't quite yet know *how*.
-------------------------------------------------------------------------------
callgrind demo
-------------------------------------------------------------------------------
- mpiexec -n 4 valgrind --tool=callgrind ./fish 1,000,000 1 30
- SLOOOOOOW!

- That's valgrind.

- mpiexec -n 4 valgrind --tool=callgrind ./fish 100,000 1 30
- look at output files

- run kcachegrind
- check measurements for moveFish, outputFish, countFish
  -> observations?
  -> why?
  -> which is 'real'?
  -> "wer misst, misst mist."

CALL TREE for moveFish:
- point out call counts, percentages

CALLEE MAP for moveFish:
- different visualization, same facts

SOURCE CODE for moveFish:
- color coding of calls
- timing on if check

SELF AND CUMULATIVE again:
- how to deal with recursion?
- point out "<cycle 1>"
- where was "<cycle 1>" in the callee map of moveFish?
- Right clickable call graph!
-------------------------------------------------------------------------------
Callgrind with caches demo:
-------------------------------------------------------------------------------
mpiexec -n 4 valgrind --tool=callgrind \
  --I1=32768,8,64 --D1=32768,8,64 --LL=262144,8,64 ./fish 100000 1 30
   (size in bytes, associativity, line size)

- point out choices
- pick "instruction fetch"
- pick "cycle estimation" -> still doesn't get real values
- pick "data read access"
- pick "L1 Data read miss"

-> source code of moveFish

- but really, countFish is disproportionate

-> source code of countFish

- Comments on HW5:

-> 95% of you who had the CS reflex: linked lists! O(1) insertion
  - Boom, wrong. Have had a good run, no more.

- Send fish one by one?
  - Good reason I didn't use your program to demonstrate with 1e6 fish
  - And what are a million fish, anyway?
  - Room for 100M in my laptop, without breaking a sweat

- Idiom: request array
  - Overrunning that?
  - Particularly effective: One fish per request

- back on topic: caches are important
- callgrind delivers exact data, slowly
-------------------------------------------------------------------------------
Perf demo
-------------------------------------------------------------------------------
- all of the above work in the VM, this one will not
- BUT: The extra speed and capabilities are totally worth it

- perf top
- point out sample count and event count

- perf list

- perf top -e 'instructions'

- response to "yes > /dev/null"

- perf record mpiexec -n 4 ./fish 1000000 1 50
- point out data file

PERF REPORT
- note how all mixed together
- hit enter on first entry
- What's a DSO?
- Zoom into thread

- annotate countFish
- move around
- follow jumps
- pick out bad instruction!

-> OF COURSE: Works less well without -g

-> REMARK: Possible to control sampling frequency

CALL GRAPHS:

- perf record -g mpiexec -n 4 ./fish 1000000 1 50
- callee or caller information?
- Expand caller info on countFish

MULTIPLE EVENTS:

- perf record -e instructions,cycles -g mpiexec -n 4 ./fish 1000000 1 50
- perf report
- perf report -n
- observe that sample counts make no sense!
-> perf is smart behind your back and adjusts the sample rate to ~1000/s
   fixes stuff up in visualization

- perf record -e instructions,cycles -c 100,000 -g mpiexec -n 4 ./fish 1000000 1 50

-> allows us to obtain meaningful ratios!

BACK TO SLIDES

- show intel arch manual
- find sandy bridge
- find branch misprediction
- perf record -e instructions,rc189 -c 10000 ./branch-mispredict

- show intel opt manual
- Appendix B.6
-------------------------------------------------------------------------------
False sharing demo
-------------------------------------------------------------------------------
ON SLATE:
- set-governor
- show code
- ./run-threads-vs-cache
- why?!
- fix it
- rerun
- "false sharing"
-------------------------------------------------------------------------------
NUMA Demo
-------------------------------------------------------------------------------
ON CRUNCHY3:
- show code
- have to pin thread -> core
  - OTHERWISE NOTHING MAKES ANY SENSE
- have to make sure memory is on determined node
  - alternative: "first touch"

- re
- ./make-on-crunchy
- show results
- discuss locality matrix
- single bw: NU for real
- contention
  - compare to zero contention case: that's like single-local
- all-contention
- two-contention
-------------------------------------------------------------------------------
Lock contention demo
-------------------------------------------------------------------------------
- set-governor
- Run with a single thread
- Put rate into perspective with sin() -- remove
- Run with 1,2,3,4,5,...,all threads
-------------------------------------------------------------------------------
GPU Branching Demo
-------------------------------------------------------------------------------
BOX
- show code
- run with 32
- run with 16 (just comment out cases)
- comment on how implemented
- run with 8
- ...
- run with 1
-------------------------------------------------------------------------------
GPU latency demo
-------------------------------------------------------------------------------
BOX:
- show code
- shrink work group size
- EXPLANATION: less latency hiding
- Use 2x ILP
  - halve size
  - halve chunksize
  - add extra fetch/store
- Use 4x ILP
-------------------------------------------------------------------------------
Arch understanding demo
-------------------------------------------------------------------------------
- What the hell is a warp?
- Why is the size of the register file given in kiB?

- mem bw
  bus bits * bus clock /1e3 / 8

-  flops per core, per clock
  fpus * 2

- flop rate [gflops]
  cores * core clock * fpus * 2 * 1e6 / 1e9

- how many scheduling slots per core?
  warp size * (# warps/core)

- how many scheduling slots total?
  * cores
  -> and that's just what the hardware does!

- how much register file per work item?
  -> "I'm going to make a mistake."
  reg file * 1024 / # fpus

  correct:
  reg file * 1024 / # work items

- smem bw / WHAT?
  -> FLOP!

  lmem bw / (#fpus * 2)

- gmem bw / flop?

  (gmem bw *1e9) / (#cores) / (core clock *1e6) / (#fpus * 2)
-------------------------------------------------------------------------------
Occupancy calculator demo
-------------------------------------------------------------------------------
Use gnumeric
- grow work group size up to 256
- observe: occupancy increases
- then increase lmem to 16384
- observe: occupancy drops

- Is occupancy absolutely necessary?
  (No. ILP can be just as good. But you pay with space in the reg.file.)
-------------------------------------------------------------------------------
Gmem access patterns demo
-------------------------------------------------------------------------------
- +i alignment on 590 -- 0 1 2 3 16
- *i strides on 295 -- 0 1 2 4 8 16 32
-------------------------------------------------------------------------------
Lmem access patterns demo
-------------------------------------------------------------------------------
FIXME!

- +i alignment on 590 -- 0 1 2 3 16
- *i strides on 295 -- 0 1 2 4 8 16 32
-------------------------------------------------------------------------------
a glimpse at MPI performance:
-------------------------------------------------------------------------------

- two key figures? latency/bw

BANDWIDTH:
- bandwidth? walk through benchmark, noting benchmarking tricks:
  "loop": run how often
  "skip": skip the warm-up
  "window-size": keep how many non-blocking requests in flight

- run just on box
  as good as 'memcpy'
  very near memory bandwidth! How many extra buffer copies can there be?
  -> point of all this buffer nonsense

  Implementation matters:
  Previous example uses shared memory between processes
  mpiexec --mca btl self,tcp -n 2
  You see the extra buffer copies!

  Now -H box,slate
  -> Networks slow
  -> cardiac,bowery use faster interconnect, I'd encourage you to try this code
     there and compare with our results

BI-BANDWIDTH:
  quick walkthrough
  full-duplex or aggregate bandwidth?

LATENCY:
  quick walkthrough
  on-host
  on-host tcp
  off-host

