-------------------------------------------------------------------------------
software installation demo
-------------------------------------------------------------------------------
- make sure to use 'name'
-------------------------------------------------------------------------------
git demo
-------------------------------------------------------------------------------
- split screen four ways

- make new dir
- git init
- (git log)
- add a.c
- commit
- git log

- edit again
- git status
- .gitignore
- add .gitignore
- git status
- the staging area (the "index")
- git add --interactive

- tig --all
- "what's this 'master' thing?"
- git branch new-feature
- make a commit
- git checkout master
- make a commit

- Blaise asks for my code
- clone
- set up tig

- merge new-feature
- make another commit

- make another commit as blaise
- git fetch ../blaises-copy master:from-blaise
- merge from-blaise

- git fetch ssh://sdfasdfasd...
- git fetch URL
- git remotes are bookmarks for these URLs
- git help <COMMAND>

- pull = fetch+merge
- push = inverse pull (but: cannot resolve conflicts,
  therefore must be fast-forward)

-------------------------------------------------------------------------------
transpose
-------------------------------------------------------------------------------
- walk through
- develop in-place transpose
- kernel
- note: need odd number of trips :)
- run with 8192

- dropped most of our mem bw
- ideas how to recover?
- contiguity of mem access?

< device lang slides

- modify to non-inplace
- modify to blocky
- modify to local w/no sync

< sync slides

- add sync
-------------------------------------------------------------------------------
intro mpi
-------------------------------------------------------------------------------
- documentation (install, "openmpi-doc", tab completion on man)

- walk through demo
- 'rank' terminology

- why no error handling?
- MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);

- just run
- mpiexec -n 5
- ("mpirun"? <- non-standard)

- mpiexec -n 4 ocean : -n 8 air

- char name[MPI_MAX_PROCESSOR_NAME];
  int namelen;
  MPI_Get_processor_name(name, &namelen);

- mpiexec -n 5 -H box,slate
- mpiexec -n 5 -H box,slate -npernode 3

- warning: -H, -npernode nonstandard
- usually: set by queuing system
  (underhanded info, qsub -I)

- show on bowery (if possible)
  qsub -l nodes=2 -q interactive -I
  module load openmpi/intel/1.6

- The typical MPI job is a batch job, run by a 'script'.
  See NYU HPC wiki under "Running jobs"

-------------------------------------------------------------------------------
mpi-send:

- MPI_Send(buf, count, type, dest, tag, comm);
- MPI_Recv(buf, count, type, dest, tag, comm, status);
  MPI_ANY_SOURCE
  MPI_ANY_TAG
  stat.MPI_SOURCE, stat.MPI_TAG

- Why does MPI need to know the type?
  - What if you'd like to send structured data?
    -> Cheat, send it as a bag of MPI_BYTE
    -> Tell MPI about the data type
      man MPI_Type_<tab>

- First transmit entire buffer.

- run just plain (-> error)

- run full

- What about variable-size data? (later!)
- MPI_Get_count(status, type, &count)

- What about truly variable-size data? (MPI_Probe)
- send too much

-------------------------------------------------------------------------------
mpi-2send:

- Write send-then-receive with 1024 buf
- Increase to 16384
- Add printf.

- Wrong mental model!
- Correct? What is the *meaning* of what we wrote?

< standard

- Fix by reordering

- n neighbor exchanges
  odd/even a solution
  what if odd number of ranks?
